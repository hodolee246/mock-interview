> 9장 정리
1. 검색 엔진 인덱싱: 크롤러의 가장 보편적인 용례로 크롤러는 웹페이지를 모아 검색 엔진을 위한 로컬 인덱스를 만든다.
2. 웹 아카이빙: 나중에 사용할 목적으로 장기보관하기 위해 웹에서 정보를 모으는 절차를 말한다. 많은 국립 도서관이 크롤러를 돌려 웹사이트를 아카이빙하고 있다.
3. 웹 마이닝: 웹의 폭발적 성장세는 데이터 마이닝 업계에 전례 없는 기회다. 웹 마이닝을 통해 인터넷에서 유용한 지식을 도출해 낼 수 있는 것이다.
4. 웹 모니터링: 크롤러를 사용하면 인터넷에서 저작권이나 상표권이 침해되는 사례를 모니터링할 수 있다.

크롤러가 만족시켜야할 속성
1. 규모 확장성
    - 병행성을 활용해 효과적은 크롤링
2. 안정성
    - 잘못된 데이터, 악성코드, 등 함정에 잘 대응
3. 예절
    - 웹 사이트에 짧은 시간동안 너무 많은 요청을 보내면 안된다 (디도스 오해)
4. 확장성
    - 새로운 형태의 콘텐츠를 지원하기 쉬워야 한다. (파일도? 특정 어떤것도? 다 해야 한다고 새롭게 설계해야 한다면 곤란할 것이다)

수집단계
1. 시작 URL 집합
2. 미수집 URL 저장소
3. HTML 다운로더 (도메인 이름 변환기)
4. 컨텐츠 파서
5. 중복 컨텐츠? <--> 컨텐츠 저장소
6. URL 추출기
7. URL 필터
8. 이미 방문한 URL?
10. URL 저장소

중요하다고 생각한 부분
- 확장과 병행을 하여 크롤링할 수 있는 설계 + 예절(크롤러는 수집 대상 웹 사이트에 짧은 시간동안 너무 많은 요청을 보내지 않도록 주의
- 중복 컨텐츠 판단 및 이미 방문한 URL은 버려 중복 및 탐색 리소스 최소화
  책과 생각이 다른 부분
  X
  개인적인 생각
- 이 챕터의 경우 아 그렇군아 하는 수준으로 넘어가는 챕터인거 같음 크롤링 또한 데이터 수집의 일종이기에 웹 개발자의 영역은 아니라고 생각함

AI
```
법적 고려사항: 크롤링 대상 사이트의 서비스 약관을 준수해야 하며, 법적 분쟁을 피하기 위한 고려가 필요하다는 점
```
